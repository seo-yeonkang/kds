import json
import os
import re
from pathlib import Path

def parse_contract_json(input_path, output_dir, max_chunk_size=1000):
    """
    ê³„ì•½ì„œ JSON íŒŒì¼ì„ RAGìš© chunkë¡œ íŒŒì‹±
    
    Args:
        input_path: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        max_chunk_size: ìµœëŒ€ chunk í¬ê¸° (ê¸°ë³¸ê°’: 1000ì)
    
    Returns:
        list: íŒŒì‹±ëœ chunks ë¦¬ìŠ¤íŠ¸
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    file_info = data['document']['metadata']['file_info']
    information = {
        'document_title': file_info['document_name'],
        'document_type': data['document']['type'],
        'document_date': file_info['document_creation_date'],
        'document_category': file_info['document_category']['sub_category']
    }
    information.update(file_info['additional_info'])
    
    # ì¡°í•­ë³„ ê·¸ë£¹í™” (is_article_titleê³¼ article í•„ë“œ í™œìš©)
    articles = {}
    misc_sections = []
    
    for section in data['document']['sections']:
        text = section['content']['description']
        format_info = section['format']
        format_type = format_info['format_type']
        is_article_title = format_info.get('is_article_title', False)
        article_num = format_info.get('article')
        
        if format_type == 'ë³¸ë¬¸' and is_article_title and article_num is not None:
            # ì¡°í•­ ì œëª© ì‹œì‘
            articles[article_num] = {
                'title': text,
                'content': [],
                'type': 'article',
                'content_labels': section['content']['content_labels']
            }
        elif format_type == 'ë³¸ë¬¸' and not is_article_title and article_num is not None:
            # is_article_titleì´ falseì´ì§€ë§Œ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¡°í•­ì¼ ìˆ˜ ìˆìŒ
            if re.match(r'^\d+\.', text.strip()) and article_num not in articles:
                # ìƒˆë¡œìš´ ì¡°í•­ ì œëª©ìœ¼ë¡œ ì²˜ë¦¬
                articles[article_num] = {
                    'title': text,
                    'content': [],
                    'type': 'article',
                    'content_labels': section['content']['content_labels']
                }
            elif article_num in articles:
                # ê¸°ì¡´ ì¡°í•­ì— ë‚´ìš© ì¶”ê°€
                articles[article_num]['content'].append(text)
            else:
                # ê¸°íƒ€ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                misc_sections.append({
                    'text': text,
                    'type': format_type,
                    'content_labels': section['content']['content_labels']
                })
        else:
            # ì„œë¬¸, ê¸°ëª…ë‚ ì¸ ë“± ê¸°íƒ€ ì„¹ì…˜
            misc_sections.append({
                'text': text,
                'type': format_type,
                'content_labels': section['content']['content_labels']
            })
    
    # ë™ì  ì²­í‚¹ í•¨ìˆ˜
    def create_dynamic_chunks(title, content_list, max_size):
        """
        ì„¸ë¶€ í•­ëª©ë“¤ì„ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê·¸ë£¹í™”
        """
        chunks = []
        current_chunk_items = []
        current_chunk_size = len(title) + 1  # ì œëª© + ì¤„ë°”ê¿ˆ
        
        for item in content_list:
            item_size = len(item)
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
            if current_chunk_size + item_size + 1 <= max_size:  # +1ì€ ì¤„ë°”ê¿ˆ
                current_chunk_items.append(item)
                current_chunk_size += item_size + 1
            else:
                # í˜„ì¬ ì²­í¬ ì™„ë£Œ
                if current_chunk_items:
                    chunk_text = title + '\n' + '\n'.join(current_chunk_items)
                    chunks.append({
                        'text': chunk_text,
                        'items': current_chunk_items.copy(),
                        'size': len(chunk_text)
                    })
                
                # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
                current_chunk_items = [item]
                current_chunk_size = len(title) + len(" (ê³„ì†)") + item_size + 2  # +2ëŠ” ì¤„ë°”ê¿ˆë“¤
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk_items:
            chunk_text = title + (" (ê³„ì†)" if chunks else "") + '\n' + '\n'.join(current_chunk_items)
            chunks.append({
                'text': chunk_text,
                'items': current_chunk_items.copy(),
                'size': len(chunk_text)
            })
        
        return chunks
    
    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì ìš©
    final_chunks = []
    
    # ë¨¼ì € ê¸°íƒ€ ì„¹ì…˜ë“¤ ì¶”ê°€ (ì„œë¬¸, ì œëª© ë“±)
    for misc in misc_sections:
        final_chunks.append({
            'text': misc['text'],
            'chunk_length': len(misc['text']),
            'type': misc['type'],
            'content_labels': misc['content_labels'],
            **information
        })
    
    # ì¡°í•­ë³„ ì²˜ë¦¬ (ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ)
    for article_num in sorted(articles.keys()):
        article_data = articles[article_num]
        title = article_data['title']
        content_list = article_data['content']
        
        if not content_list:
            # ì œëª©ë§Œ ìˆëŠ” ê²½ìš°
            final_chunks.append({
                'text': title,
                'article_number': article_num,
                'article_title': title,
                'chunk_length': len(title),
                'type': 'article',
                'content_labels': article_data['content_labels'],
                **information
            })
        else:
            # ë™ì  ì²­í‚¹ ì ìš©
            article_chunks = create_dynamic_chunks(title, content_list, max_chunk_size)
            
            for i, chunk_data in enumerate(article_chunks):
                chunk_info = {
                    'text': chunk_data['text'],
                    'article_number': article_num,
                    'article_title': title,
                    'chunk_part': i + 1,
                    'total_parts': len(article_chunks),
                    'chunk_length': chunk_data['size'],
                    'items_count': len(chunk_data['items']),
                    'type': 'article',
                    'content_labels': article_data['content_labels'],
                    **information
                }
                final_chunks.append(chunk_info)
    
    # ì €ì¥
    document_name = information['document_title']
    output_path = os.path.join(output_dir, f"{document_name}_parsed.json")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_chunks, f, ensure_ascii=False, indent=2)
    
    return final_chunks

def batch_parse_contracts(input_dir, output_dir, max_chunk_size=1000):
    """
    ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSON íŒŒì¼ì„ ë°°ì¹˜ ì²˜ë¦¬
    
    Args:
        input_dir: ì…ë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        max_chunk_size: ìµœëŒ€ chunk í¬ê¸° (ê¸°ë³¸ê°’: 1000ì)
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    input_path = Path(input_dir)
    json_files = list(input_path.glob("*.json"))
    
    if not json_files:
        print(f"âŒ {input_dir}ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    print(f"ğŸ“ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    results = {
        'total_files': len(json_files),
        'processed_files': 0,
        'failed_files': 0,
        'total_chunks': 0,
        'failed_list': []
    }
    
    for json_file in json_files:
        try:
            print(f"ğŸ”„ ì²˜ë¦¬ì¤‘: {json_file.name}")
            chunks = parse_contract_json(str(json_file), output_dir, max_chunk_size)
            results['processed_files'] += 1
            results['total_chunks'] += len(chunks)
            print(f"âœ… {json_file.name}: {len(chunks)}ê°œ chunks ìƒì„±")
            
        except Exception as e:
            print(f"âŒ {json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            results['failed_files'] += 1
            results['failed_list'].append({
                'file': json_file.name,
                'error': str(e)
            })
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ ê²°ê³¼:")
    print(f"   - ì´ íŒŒì¼ ìˆ˜: {results['total_files']}")
    print(f"   - ì„±ê³µ: {results['processed_files']}")
    print(f"   - ì‹¤íŒ¨: {results['failed_files']}")
    print(f"   - ì´ chunks: {results['total_chunks']}")
    
    if results['failed_list']:
        print(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for failed in results['failed_list']:
            print(f"   - {failed['file']}: {failed['error']}")
    
    return results

if __name__ == "__main__":
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    input_directory = "../data/training_labeling"
    output_directory = "../data/rag_labeling"
    
    print("ğŸš€ ê³„ì•½ì„œ JSON íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_directory}")
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_directory}")
    print(f"ğŸ“ ìµœëŒ€ ì²­í¬ í¬ê¸°: 1000ì")
    print("-" * 50)
    
    results = batch_parse_contracts(input_directory, output_directory, max_chunk_size=1000)
    
    print("\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!") 